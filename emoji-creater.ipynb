{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "%pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --default-timeout=1000 future --use-pep517"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tensorflow --default-timeout=10000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.sys.path\n",
        "# add open cv to path\n",
        "os.sys.path.append('/usr/local/lib/python3.10/dist-packages')    \n",
        "os.sys.path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_UGyBEwj04qC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "#from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "#from keras.utils import plot_model\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "#from livelossplot import PlotLossesKerasTF\n",
        "from keras.utils import plot_model\n",
        "import scipy\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sJtuLAyr1Eyd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ],
      "source": [
        "train_dir = 'train'\n",
        "val_dir = 'test'\n",
        "img_size=48\n",
        "batch_size=64\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(img_size,img_size),\n",
        "        batch_size=batch_size,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(img_size,img_size),\n",
        "        batch_size=batch_size,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8rUU4B0l1F1X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3995 angry images\n",
            "436 disgust images\n",
            "4097 fear images\n",
            "7215 happy images\n",
            "4965 neutral images\n",
            "4830 sad images\n",
            "3171 surprise images\n"
          ]
        }
      ],
      "source": [
        "for i in os.listdir(\"train/\"):\n",
        "    print(str(len(os.listdir(\"train/\"+i))) +\" \"+ i +\" images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jr4X2_CL1JSl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "958 angry images\n",
            "111 disgust images\n",
            "1024 fear images\n",
            "1774 happy images\n",
            "1233 neutral images\n",
            "1247 sad images\n",
            "831 surprise images\n"
          ]
        }
      ],
      "source": [
        "for i in os.listdir(\"test/\"):\n",
        "    print(str(len(os.listdir(\"test/\"+i))) +\" \"+ i +\" images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jCtvUJAh1LZH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 46, 46, 32)        320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 44, 44, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 22, 22, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 22, 22, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 20, 20, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 10, 10, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              2098176   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7)                 7175      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,345,607\n",
            "Trainable params: 2,345,607\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "emotion_model = Sequential()\n",
        "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))#output=(48-3+0)/1+1=46\n",
        "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))#output=(46-3+0)/1+1=44\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))#output=devided input by 2 it means 22,22,64\n",
        "emotion_model.add(Dropout(0.25))#reduce 25% module at a time of output\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',input_shape=(48,48,1)))#(22-3+0)/1+1=20\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))#10\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))#(10-3+0)/1+1=8\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))#output=4\n",
        "emotion_model.add(Dropout(0.25))#nothing change\n",
        "emotion_model.add(Flatten())#here we get multidimention output and pass as linear to the dense so that 4*4*128=2048\n",
        "emotion_model.add(Dense(1024, activation='relu'))#hddien of 1024 neurons of input \n",
        "emotion_model.add(Dropout(0.5))\n",
        "emotion_model.add(Dense(7, activation='softmax'))#hddien of 7 neurons of input\n",
        "plot_model(emotion_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)#save model leyer as model_plot.png\n",
        "emotion_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFrSKjJ31PBC"
      },
      "outputs": [],
      "source": [
        "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "emotion_model_info = emotion_model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=448,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=112)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SYUljMUd1ReR"
      },
      "outputs": [],
      "source": [
        "emotion_model.save_weights('new_model20.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load emotion model\n",
        "emotion_model.load_weights('model50.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stRzOlEK1UlE"
      },
      "outputs": [],
      "source": [
        "# start the webcam feed\n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Natural\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "\n",
        "#cap = cv2.VideoCapture('facial_exp.mkv')\n",
        "cap = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    # Find haar cascade to draw bounding box around face\n",
        "    ret, frame = cap.read()\n",
        "    #frame = cv2.flip(frame, 1)\n",
        "    if not ret:\n",
        "        break\n",
        "    bounding_box = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    for (x, y, w, h) in num_faces:\n",
        "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
        "        emotion_prediction = emotion_model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(emotion_prediction))\n",
        "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    cv2.imshow('Video', cv2.resize(frame,(600,500),interpolation = cv2.INTER_CUBIC))\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTTZQ6Io1XC7"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('logmodel50.pkl', 'wb') as fid:\n",
        "    pickle.dump(emotion_model, fid,2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3uA7C7921Zzr"
      },
      "outputs": [],
      "source": [
        "model_json=emotion_model.to_json()\n",
        "with open(\"new_model15.json\",\"w\")as json_file:\n",
        "    json_file.write(model_json)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "049f661d90168e075950bd622321264b76fd76293b9fd95adf920aec4962354f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
